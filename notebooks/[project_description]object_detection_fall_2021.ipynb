{"cells":[{"cell_type":"markdown","metadata":{"id":"dV66G0WGGuBj"},"source":["<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=500></p>\n","\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"9LuFLCm3GuBl"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"z7pPmypAGuBm"},"source":["<h2 style=\"text-align: center;\"><b>Object detection</b></h2>"]},{"cell_type":"markdown","metadata":{"id":"QLt9nwPoGuBn"},"source":["### Руководители проекта: \n","* Илья Захаркин (Samsung AI Center - Moscow) | tg:@izakharkin\n","* Юрий Яровиков (AIRI, МФТИ) | tg:@yu_rovikov"]},{"cell_type":"markdown","metadata":{"id":"Y_nFVirpGuBo"},"source":["<p style=\"text-align: center;\"><img src=\"https://miro.medium.com/max/750/1*LTPAzUjUentTWoXXmqoT9g.jpeg\"></p>"]},{"cell_type":"markdown","metadata":{"id":"IwjXeeaDGuBp"},"source":["<h2 style=\"text-align: center;\"><b>Этапы работы</b></h2>"]},{"cell_type":"markdown","metadata":{"id":"8cLcEEnVGuBq"},"source":["Проект подразумевает выполнение всеми участниками пунктов 1-2, и далее выбор каждым слушателем своего сценария -- 1 или 2. Можно сделать оба сценария, то есть обучить модель на своих данных и её же встроить в демо, но для этого стоит быть готовым к удвоению затрачиваемого на проект времени."]},{"cell_type":"markdown","metadata":{"id":"rQKgMK4KGuBr"},"source":["<h3 style=\"text-align: center;\"><b>Общий этап работы</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"7go9drOyoTSp"},"source":["**Hint**: если у вас на компьютере нет GPU, то лучше с самого начала делать все в Google Colab."]},{"cell_type":"markdown","metadata":{"id":"ZFQCbhgIGuBr"},"source":["**1). Выбор фреймворка/библиотеки для использования детектора**\n","\n","Чтобы освежить память о задаче детекции, можно посмотреть [занятия на продвинутом курсе](https://stepik.org/lesson/458312/step/1?unit=616130).\n","\n","В выборе фреймворка предоставляется свобода, лично я рекомендовал бы один из:\n","- `torchvision.models.detection` и `torchhub`: \"нативные\" модели для детектирования прямо из PyTorch. Примеры использования есть прямо на занятиях DLSchool по практике CV [2019 года](https://www.youtube.com/watch?v=XSPYe4-y4HE) и [2020 года](https://stepik.org/lesson/458313/step/1?unit=616131);\n","- `mmdetection`: как с ним работать, рассказывается в [практическом занятии](https://stepik.org/lesson/458313/step/2?unit=616131).\n","- `detectron2`: краткая информация есть в конце [занятия DLSchool по практике CV](https://www.youtube.com/watch?v=XSPYe4-y4HE), можно начать с него. Лучше самостоятелньо изучить [официальный репозиторий](https://github.com/facebookresearch/detectron2) и уже с ним работать в дальнейшем (\"Quick Start\");\n","- `TensorFlow Object Detection API`: как с ним работать рассказывается в [занятии 2018 года](https://www.youtube.com/watch?v=xHIzyrU1uVM). Работать предстоит с [официальным репозиторием](https://github.com/tensorflow/models/tree/master/research/object_detection)."]},{"cell_type":"markdown","metadata":{"id":"XUo1cchSGuBs"},"source":["Рекомендую выбрать один из них и работать уже с этим модулем."]},{"cell_type":"markdown","metadata":{"id":"QMBzevfKGuBt"},"source":["> Результатом пункта является зафиксированный фреймворк для нейросети-детектора."]},{"cell_type":"markdown","metadata":{"id":"SKZGkUpCGuBt"},"source":["**2). Запуск детектора на случайных изображениях**\n","\n","Этот пункт просто про то, чтобы запустить любую модель детектирования в выбранном выше репозитории. Таким образом, часть с запуском будет работать, и далее уже можно приступать к основным сценариям."]},{"cell_type":"markdown","metadata":{"id":"bRVENAk0GuBu"},"source":["> Результатом пункта явлется набор изображений, на которых модель успешно отработала и результат детекции виден и понятен."]},{"cell_type":"markdown","metadata":{"id":"2LTEStsXGuBv"},"source":["<h3 style=\"text-align: center;\"><b>1 сценарий</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"kx3MZ2v4GuBv"},"source":["В первом сценарии упор делается на разработку демо, в которое встривается нейросетевая модель для детекции."]},{"cell_type":"markdown","metadata":{"id":"DbRJ7T6QGuBw"},"source":["Если у Вас есть опыт веб- или мобильной разработки, то можете использовать именного его и работать в рамках привычных Вам инструментов. Главное, чтобы в итоге они позволяил встроить в себя нейросетевой детектор, на вход которому будут поступать картинки.\n","\n","Изображения на вход демо могут поступать с веб-камеры, из файлов, по ссылке или с камеры мобильного телефона -- способ может быть любой (на Ваш выбор). Демо должно показывать, что детектор успешно отрабатывает на поданных изображениях и находит нужные объекты. Оформлять красиво визуальную часть демо -- приветствуется, но не обязательно -- это может быть просто кнопка \"загрузить\" и показ фото до/после детекции."]},{"cell_type":"markdown","metadata":{"id":"lczuLKgpGuBx"},"source":["**3). Выбор фреймворка/библиотеки для разработки веб/мобильного демо**\n","\n","Основным инструментом для разработки веб-демо будет микрофреймворк **Flask**: [серия туториалов на русском](https://habr.com/ru/post/346306/).   \n","Полезные ресуры:\n","- [курс по веб-разработке](https://www.youtube.com/playlist?list=PLzQrZe3EemP5KsgWGnmC0QrOzQqjg3Kd5), нас интересуют первые 7 видео в плейлисте. В частности, нужны видео по Flask, там очень хорошие обучалки параллельно с лектором;\n","- [исчерпывающий справочник по Flask (англ)](https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python3_Flask.html);\n","- можно посмотреть мой [репозиторий с реализацией веб-демо](https://github.com/izaharkin/Respalyzer) для ML-задачи оценки отзывов.\n","\n","Для разработки мобильного демо стоит выбрать инстурмент на свое усмотрение:\n","- под Android: [пример на Pytorch Mobile](https://towardsdatascience.com/object-detector-android-app-using-pytorch-mobile-neural-network-407c419b56cd), [пример на TensorFlow Lite](https://www.tensorflow.org/lite/models/object_detection/overview). **Примечание** от Дмитрия Шумилина: на Android с TF Lite на момент января 2021 есть [ошибка](https://github.com/tensorflow/models/issues/9341) с новым форматом хранения модели. Можно попробовать возможное [решение](https://www.youtube.com/watch?v=syTKGY-H44E&ab_channel=DoomsdayRobotics) или писать на чистом Java. Также можно попробовать использовать более старые версии TensorFlow, в которых проблем совместимости еще не было, например, [v2.1.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0).\n","- под iOS: [пример на TensorFlow Lite Swift API](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_swift), [пример с Vision Framework](https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture) на \"чистом\" Swift'е.\n","\n","Разумеется, лучше **самостоятельно поискать видео/статьи** на тему использования моделей на мобильных устройствах."]},{"cell_type":"markdown","metadata":{"id":"fpGAvez5GuBx"},"source":["> Результатом пункта является зафисированный для вас инстурмент для разработки демо."]},{"cell_type":"markdown","metadata":{"id":"ggyC5X6nGuBy"},"source":["**4). Разработка демо**"]},{"cell_type":"markdown","metadata":{"id":"zqsnd7mkGuBy"},"source":["Этот пункт про сам процесс написания кода для демо. \n","\n","> Результатом пункта является код, который можно запустить. Не хватать будет только логики детектора, сам интерфейс должен быть уже рабочим."]},{"cell_type":"markdown","metadata":{"id":"_HbHX0oAGuBz"},"source":["**5). Встраивание модели-детектора в демо**"]},{"cell_type":"markdown","metadata":{"id":"bUgXk1UBGuB0"},"source":["Этот пункт про процесс дописывания кода, который будет обеспечивать \"логику\" демо $-$ само детектирование.\n","\n","> Результатом пункта является код, который можно запустить и продемонстрировать работающую систему детектирования объектов."]},{"cell_type":"markdown","metadata":{"id":"hC57dH9sGuB0"},"source":["**6). Тестирование демо**"]},{"cell_type":"markdown","metadata":{"id":"BbuoADSeGuB1"},"source":["Здесь нужно запустить ваше демо на как можно большем количестве примеров, чтобы понять, в чем его сильные и слабые стороны. То есть какие объекты/сцены детектор обрабатывает легко, а с какими ему справится сложно. Нужно предложить также пути для улучшения модели на основе увиденных ошибок."]},{"cell_type":"markdown","metadata":{"id":"3-_tW90YGuB1"},"source":["> Результатом пункта является набор изображений, на которых демо отработало. Для каждого изображения нужно добавить комментарии, почему модель справилась хорошо/плохо, предложить пути ее улучшения."]},{"cell_type":"markdown","metadata":{"id":"vQ_AQ1MMGuB2"},"source":["**7). Оформление демо для показа другим людям**"]},{"cell_type":"markdown","metadata":{"id":"YmTEuUxKGuB3"},"source":["В этом пункте можно пойти двумя путями:\n","1. Проделать работу по улучшению визуальной составляющей демо (интерфейс)\n","2. Загрузить модель на какой-нибудь сервер/хост/test-flight (в случае мобильного iOS-демо), чтобы к демо можно было обратиться прямо в адресной строке браузера\n","\n","\\> По *первому пункту* могу посоветовать использовать библиотеку [Bootstrap](https://habr.com/ru/post/349060/), для мобильного демо элементы UI/UX являются частью основной разработки (поэтому стоит просто погуглить/почитать документацию).\n","\n","\\> *Второй пункт - в случае веб-демо*: \n","\n","Способ 1: Google Cloud Engine.\n","\n","Если ваше приложение требует установки системных пакетов, например, через `apt-get install`, то вам придется работать на выделенном сервере VPN или на виртуальной машине. К счастью тот же [Google Cloud](https://cloud.google.com/compute) предоставляет бесплатные 300$ на 90 дней использования Виртуальной машиной, чего хватит в большинстве случаев. Эти ссылки помогут вам понять, как в таком случае создать виртуальную машину, установить и настроить виртуальное окружение и вебсервер, а также задеплоить проект:\n","\n","- [Deploying a Flask app to a Virtual Machine](https://www.youtube.com/watch?v=a2g9pDleGQk&ab_channel=JulianNash)\n","- [Set up Gunicorn and Nginx](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-20-04-ru)\n","\n","Способ 2: Heroku.\n","\n","Если с GCE проблемы/не хочется привязывать карту и т.д., могут помочь эти ресурсы и сервис [Heroku](https://www.heroku.com/):\n","- [Flask deployment](http://www.tutorialspoint.com/flask/flask_deployment.htm)\n","- [Deploy Flask app to Heroku (youtube)](https://www.youtube.com/watch?v=pmRT8QQLIqk)\n","- [Deploy Flask app to Heroku (medium)](https://medium.com/the-andela-way/deploying-your-flask-application-to-heroku-c99050bce8f9)\n","- [Set your own domain name on Heroku](https://devcenter.heroku.com/articles/custom-domains)\n","\n","\\> *Второй пункт - в случае мобильного демо*: \n","\n","Здесь с как таковым деплоем сложнее, обычно мобильные приложения публикуются или в Google Play (Android), или в AppStore (iOS). Однако можно снять **видеопоказ экрана (скринкаст)** с использованием написанного приложения - вполне подойдет для публичной демонстрации."]},{"cell_type":"markdown","metadata":{"id":"jt-XtoKSGuB3"},"source":["<h3 style=\"text-align: center;\"><b>2 сценарий</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"9qbxIecoGuB4"},"source":["Во втором сценарии упор делается на сбор выборки, преобработку данных, обучение модели и измерение качества её работы. То есть осуществляется полный цикл разработки нейросетевой модели для задачи детектирования без встривания в веб/мобильный прототип."]},{"cell_type":"markdown","metadata":{"id":"bNm2OZMcGuB4"},"source":["**3). Выбор датасета**"]},{"cell_type":"markdown","metadata":{"id":"90z-NuSbGuB5"},"source":["При работе с датасетом вы неизбежно столкнетесь с работой с файлами и папками (директориями). Рекомендуется освежить в памяти работу с библиотеками `os`, `json`, `glob`. Может помочь [этот туториал](https://realpython.com/working-with-files-in-python/).\n","\n","На выбор предоставляются 5 датасетов по детекции объектов:\n","1. [Детекция игровых карт](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10): лежат в папке images вместе с разметкой;\n","2. [Детекция фруктов](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection): скачать можно, нажав на кнопку Download;\n","3. [Детекция одежды (Deep Fashion 2)](https://github.com/switchablenorms/DeepFashion2): стоит прочитать README на главной странице репозитория. Для получения датасета нужно запросить пароль у автора через гугл-форму. После скачивания распакуйте его с использованием пароля. Из файлов аннотаций нас будут интересовать только `bounding_box`, `category_name` и `category_id`;\n","4. [Детекция лиц (Wider Face)](http://shuoyang1213.me/WIDERFACE/): большой датасет для детектирования лиц самых разных размеров. Скачать можно прямо по ссылкам на сайте;\n","5. [Детекция лиц (Kaggle)](https://www.kaggle.com/dataturks/face-detection-in-images): в датасете достаточно мало данных, но можно попробовать, если датасеты выше показались неподходящими для Вас;\n","6. Датасет из любого соревновани по детекции на Kaggle."]},{"cell_type":"markdown","metadata":{"id":"D_11gucPGuB6"},"source":["> Результатом выполнения пункта явлется загруженный датасет, состоящий из изображений и разметки к ним (bounding box'ов всех объектов на каждом изображении)."]},{"cell_type":"markdown","metadata":{"id":"b-Yz0JAWGuB6"},"source":["**4). Предобработка данных**\n","\n","Самый непростой этап в этом сценарии. Скачать данные $-$ лишь половина дела. Чтобы обучить нейросеть на этих данных, нужно написать генератор батчей. Однако если будем подавать изображения так, как они есть, то даже батч собрать не сможем -- нужно привести их к однмоу размеру. Далее нужно привести их к типу float, переместить на CUDA и поделить значения в пикселях на 255 (подробнее см. [занятие](https://www.youtube.com/watch?v=XSPYe4-y4HE)). Также нужно настроить аугментации и постобработку.\n","\n","То, как именно все это реализовать $-$ зависит от инструмента, выбранного в пункте 1. Например, в detectron2 в обучающих материалах описан формат данных для обучения. Возможно, нужно будет зайти в документацию и почитать более подробно, чтобы разобраться, какой именно нужен формат координат.\n","\n","НЕ нужно копировать все файлы с картинками и разметкой прямо на диске в их предобработанные версии. Хороший тон $-$ осуществлять всю эту обработку программно, \"на лету\". Поможет [туториал](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) по написанию своего датасета на PyTorch.\n","\n","> Результатом выполнения пункта явлется код, запуск которого ведет к подаче батчей правильного вида (разметка приведена к требуемому формату координат, изображения нужного типа, размера и поделены на 255 и т.д.) для обучения нейронной сети-детектора."]},{"cell_type":"markdown","metadata":{"id":"3kOKLO9FGuB7"},"source":["**5). Обучение модели-детектора** \n","\n","Стоит написать цикл обучения на PyTorch самостоятельно, однако допускается использовать [Catalyst](https://github.com/catalyst-team/catalyst), [PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning) или [Ignite](https://github.com/pytorch/ignite). Например, саму модель можно взять из фреймворка из пункта 1, а обучение осуществлять с помощью одной перечисленных трех библиотек."]},{"cell_type":"markdown","metadata":{"id":"2XAXboBIGuB7"},"source":["> Результатом выполнения пункта явлется код, запуск которого ведет к обучению модели на выбранном датасете. При обучении **обязательно выводить числовые значения лосса на трейне и валидации**, крайне желательно использовать [`TensorBoard`](https://pytorch.org/docs/stable/tensorboard.html) для визуализации. Обязательно также сохранять модель после каждой N-ой эпохи, чтобы потом ее качество можно было проверить и веса были переиспользуемыми."]},{"cell_type":"markdown","metadata":{"id":"LyZ2CgtoGuB8"},"source":["**6). Измерение качества работы модели (метрики согласуются с руководителем и зависят от задачи)**"]},{"cell_type":"markdown","metadata":{"id":"agZ_BOjEGuB9"},"source":["Под метриками понимаются функции/формулы, по которым оценивается качество модели-детектора. Обычно для измерения качества работы детектора используют поклассовые Precision, Recall, F1-меру и mean Average Precision (mAP). Подробнее про них можно послушать в [видеолекции 2018 года](https://www.youtube.com/watch?v=ewkSI2cuyoQ&list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&index=24).\n","\n","Необходимо самостоятельно реализовать функции, вычисляющие поклассовые Precision и Recall. На вход функциям поступают веса модели и выборка, на которой нужно измерить метрики. На выходе $-$ таблица с precision и recall для каждого класса.\n","\n","> Результат пункта $-$ реализованные функции метрик для задачи детектирования, позволяющие оценить качество работы модели на выборке. Выборка/даталоадер является аргументом функций."]},{"cell_type":"markdown","metadata":{"id":"mnvIv-rbGuB9"},"source":[" **7). Поиск путей применения этой модели в бизнесе/реальных задачах/набросок встраивания в веб/мобильное демо**\n"," \n","В этом пункте нужно подумать, как эта модель может быть использована в дальнейшем. То есть, например, зачем нужно детектировать фрукты? Или одежду?\n","\n","> Результат пункта $-$ перечисленные кейсы использования модели (описанные **как можно подробнее**).\n","\n","**IMPORTANT NOTE:** Обычно этим вопросом все же задаются до начала какой-либо разработки. Просто в данном случае проект выполняется скорее в образовательных целях."]},{"cell_type":"markdown","metadata":{"id":"G50tgxC9GuB-"},"source":["<h2 style=\"text-align: center;\"><b>Критерии оценивания</b></h2>"]},{"cell_type":"markdown","metadata":{"id":"Nt7FI0JdGuB-"},"source":["* 1 пункт $-$ 1 балл \n","* 2 пункт $-$ 1 балл \n","* 3 пункт $-$ 0 баллов (промежуточный пункт)  \n","* 4 пункт $-$ 3 балла   \n","* 5 пункт $-$ 3 балла   \n","* 6 пункт $-$ 1 балл \n","* 7 пункт $-$ 1 балл \n","* Максимум баллов по проекту $-$ 10  "]},{"cell_type":"markdown","metadata":{"id":"JZxROjYgGuB_"},"source":["**Успехов в выполнении проекта!** \n","\n","Желаю всем проделать полезную, интересную и качественную работу, которую потом нестыдно и в резюме указать, и друзьям показать ;)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1LZvWZzbUWIlQQR85kBtLRyZ60lCSNIJA","timestamp":1674026677501}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
